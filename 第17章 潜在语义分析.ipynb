{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.单词向量空间模型通过单词的向量表示文本的语义内容。以单词-文本矩阵$X$为输入，其中每一行对应一个单词，每一列对应一个文本，每一个元素表示单词在文本中的频数或权值（如TF-IDF）\n",
    "$$X = \\left[ \\begin{array} { c c c c } { x _ { 11 } } & { x _ { 12 } } & { \\cdots } & { x _ { 1 n } } \\\\ { x _ { 21 } } & { x _ { 22 } } & { \\cdots } & { x _ { 2 n } } \\\\ { \\vdots } & { \\vdots } & { } & { \\vdots } \\\\ { x _ { m 1 } } & { x _ { m 2 } } & { \\cdots } & { x _ { m n } } \\end{array} \\right]$$\n",
    "单词向量空间模型认为，这个矩阵的每一列向量是单词向量，表示一个文本，两个单词向量的内积或标准化内积表示文本之间的语义相似度。\n",
    "\n",
    "2.话题向量空间模型通过话题的向量表示文本的语义内容。假设有话题文本矩阵$$Y = \\left[ \\begin{array} { c c c c } { y _ { 11 } } & { y _ { 12 } } & { \\cdots } & { y _ { 1 n } } \\\\ { y _ { 21 } } & { y _ { 22 } } & { \\cdots } & { y _ { 2 n } } \\\\ { \\vdots } & { \\vdots } & { } & { \\vdots } \\\\ { y _ { k 1 } } & { y _ { k 2 } } & { \\cdots } & { y _ { k n } } \\end{array} \\right]$$\n",
    "其中每一行对应一个话题，每一列对应一个文本每一个元素表示话题在文本中的权值。话题向量空间模型认为，这个矩阵的每一列向量是话题向量，表示一个文本，两个话题向量的内积或标准化内积表示文本之间的语义相似度。假设有单词话题矩阵$T$\n",
    "$$T = \\left[ \\begin{array} { c c c c } { t _ { 11 } } & { t _ { 12 } } & { \\cdots } & { t _ { 1 k } } \\\\ { t _ { 21 } } & { t _ { 22 } } & { \\cdots } & { t _ { 2 k } } \\\\ { \\vdots } & { \\vdots } & { } & { \\vdots } \\\\ { t _ { m 1 } } & { t _ { m 2 } } & { \\cdots } & { t _ { m k } } \\end{array} \\right]$$ \n",
    "其中每一行对应一个单词，每一列对应一个话题，每一个元素表示单词在话题中的权值。\n",
    "\n",
    "给定一个单词文本矩阵$X$\n",
    "$$X = \\left[ \\begin{array} { c c c c } { x _ { 11 } } & { x _ { 12 } } & { \\cdots } & { x _ { 1 n } } \\\\ { x _ { 21 } } & { x _ { 22 } } & { \\cdots } & { x _ { 2 n } } \\\\ { \\vdots } & { \\vdots } & { } & { \\vdots } \\\\ { x _ { m 1 } } & { x _ { m 2 } } & { \\cdots } & { x _ { m n } } \\end{array} \\right]$$\n",
    "\n",
    "潜在语义分析的目标是，找到合适的单词-话题矩阵$T$与话题文本矩阵$Y$，将单词文本矩阵$X$近似的表示为$T$与$Y$的乘积形式。\n",
    "$$X \\approx T Y$$\n",
    "\n",
    "等价地，潜在语义分析将文本在单词向量空间的表示X通过线性变换$T$转换为话题向量空间中的表示$Y$。\n",
    "\n",
    "潜在语义分析的关键是对单词-文本矩阵进行以上的矩阵因子分解（话题分析）\n",
    "\n",
    "3.潜在语义分析的算法是奇异值分解。通过对单词文本矩阵进行截断奇异值分解，得到\n",
    "$$X \\approx U _ { k } \\Sigma _ { k } V _ { k } ^ { T } = U _ { k } ( \\Sigma _ { k } V _ { k } ^ { T } )$$\n",
    "\n",
    "矩阵$U_k$表示话题空间，矩阵$( \\Sigma _ { k } V _ { k } ^ { T } )$是文本在话题空间的表示。\n",
    "\n",
    "4.非负矩阵分解也可以用于话题分析。非负矩阵分解将非负的单词文本矩阵近似分解成两个非负矩阵$W$和$H$的乘积，得到\n",
    "$$X \\approx W H$$\n",
    "\n",
    "矩阵$W$表示话题空间，矩阵$H$是文本在话题空间的表示。\n",
    "\n",
    "非负矩阵分解可以表为以下的最优化问题：\n",
    "$$\\left. \\begin{array} { l } { \\operatorname { min } _ { W , H } \\| X - W H \\| ^ { 2 } } \\\\ { \\text { s.t. } W , H \\geq 0 } \\end{array} \\right.$$\n",
    "非负矩阵分解的算法是迭代算法。乘法更新规则的迭代算法，交替地对$W$和$H$进行更新。本质是梯度下降法，通过定义特殊的步长和非负的初始值，保证迭代过程及结果的矩阵$W$和$H$均为非负。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 单词向量空间  \n",
    "word vector space model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "给定一个文本，用一个向量表示该文本的”语义“， 向量的**每一维对应一个单词**，其数值为该单词在该文本中出现的频数或权值；基本假设是文本中所有单词的出现情况表示了文本的语义内容，文本集合中的每个文本都表示为一个向量，存在于一个向量空间；向量空间的度量，如内积或标准化**内积**表示文本之间的**相似度**。\n",
    "\n",
    "给定一个含有$n$个文本的集合$D=({d_{1}, d_{2},...,d_{n}})$，以及在所有文本中出现的$m$个单词的集合$W=({w_{1},w_{2},...,w_{m}})$. 将单词在文本的出现的数据用一个单词-文本矩阵（word-document matrix）表示，记作$X$:\n",
    "\n",
    "$\n",
    "X = \\begin{bmatrix}\n",
    "x_{11} &  x_{12}&  x_{1n}& \\\\ \n",
    "x_{21}&  x_{22}&  x_{2n}& \\\\ \n",
    "\\vdots &  \\vdots &  \\vdots & \\\\ \n",
    "x_{m1}&  x_{m2}&  x_{mn}& \n",
    "\\end{bmatrix}\n",
    "$\n",
    "\n",
    "这是一个$m*n$矩阵，元素$x_{ij}$表示单词$w_{i}$在文本$d_{j}$中出现的频数或权值。由于单词的种类很多，而每个文本中出现单词的种类通常较少，所有单词-文本矩阵是一个稀疏矩阵。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "权值通常用单词**频率-逆文本率**（term frequency-inverse document frequency, TF-IDF）表示：\n",
    "\n",
    "$TF-IDF(t, d ) = TF(t, d) * IDF(t)$,  \n",
    "\n",
    "其中，$TF(t,d)$为单词$t$在文本$d$中出现的概率，$IDF(t)$是逆文本率，用来衡量单词$t$对表示语义所起的重要性， \n",
    "\n",
    "$IDF(t) = log(\\frac{len(D)}{len(t \\in D) + 1})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "单词向量空间模型的优点是**模型简单，计算效率高**。因为单词向量通常是稀疏的，单词向量空间模型也有一定的局限性，体现在内积相似度未必能够准确表达两个文本的语义相似度上。因为自然语言的单词具有一词多义性（polysemy）及多词一义性（synonymy）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2, 0, 0, 0],\n",
       "       [0, 2, 0, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 2, 3],\n",
       "       [0, 0, 0, 1],\n",
       "       [1, 2, 2, 1]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = [[2, 0, 0, 0], [0, 2, 0, 0], [0, 0, 1, 0], [0, 0, 2, 3], [0, 0, 0, 1], [1, 2, 2, 1]]\n",
    "X = np.asarray(X);X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 奇异值分解\n",
    "U,sigma,VT=np.linalg.svd(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-7.84368672e-02,  2.84423033e-01,  8.94427191e-01,\n",
       "         2.15138396e-01, -2.68931121e-02, -2.56794523e-01],\n",
       "       [-1.56873734e-01,  5.68846066e-01, -4.47213595e-01,\n",
       "         4.30276793e-01, -5.37862243e-02, -5.13589047e-01],\n",
       "       [-1.42622354e-01, -1.37930417e-02, -6.93889390e-17,\n",
       "        -6.53519444e-01,  4.77828945e-01, -5.69263078e-01],\n",
       "       [-7.28804669e-01, -5.53499910e-01,  5.55111512e-17,\n",
       "         1.56161345e-01, -2.92700697e-01, -2.28957508e-01],\n",
       "       [-1.47853320e-01, -1.75304609e-01, -6.93889390e-18,\n",
       "         4.87733411e-01,  8.24315866e-01,  1.73283476e-01],\n",
       "       [-6.29190197e-01,  5.08166890e-01,  1.11022302e-16,\n",
       "        -2.81459486e-01,  5.37862243e-02,  5.13589047e-01]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.47696617, 2.7519661 , 2.        , 1.17620428])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.75579600e-01, -3.51159201e-01, -6.38515454e-01,\n",
       "        -6.61934313e-01],\n",
       "       [ 3.91361272e-01,  7.82722545e-01, -3.79579831e-02,\n",
       "        -4.82432341e-01],\n",
       "       [ 8.94427191e-01, -4.47213595e-01, -5.55111512e-17,\n",
       "        -8.32667268e-17],\n",
       "       [ 1.26523351e-01,  2.53046702e-01, -7.68672366e-01,\n",
       "         5.73674125e-01]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TruncatedSVD(algorithm='randomized', n_components=3, n_iter=7, random_state=42,\n",
       "             tol=0.0)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svd = TruncatedSVD(n_components=3, n_iter=7, random_state=42)\n",
    "svd.fit(X)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.39945801 0.34585056 0.18861789]\n"
     ]
    }
   ],
   "source": [
    "print(svd.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.933926460028448\n"
     ]
    }
   ],
   "source": [
    "print(svd.explained_variance_ratio_.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.47696617 2.7519661  2.        ]\n"
     ]
    }
   ],
   "source": [
    "print(svd.singular_values_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 非负矩阵分解"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def inverse_transform(W, H):\n",
    "    # 重构\n",
    "    return W.dot(H)\n",
    "\n",
    "def loss(X, X_):\n",
    "    #计算重构误差\n",
    "    return ((X - X_) * (X - X_)).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 算法 17.1\n",
    "\n",
    "class MyNMF:\n",
    "    def fit(self, X, k, t):\n",
    "        m, n = X.shape\n",
    "        \n",
    "        W = np.random.rand(m, k)\n",
    "        W = W/W.sum(axis=0)\n",
    "        \n",
    "        H = np.random.rand(k, n)\n",
    "        \n",
    "        i = 1\n",
    "        while i < t:\n",
    "            \n",
    "            W = W * X.dot(H.T) / W.dot(H).dot(H.T)\n",
    "            \n",
    "            H = H * (W.T).dot(X) / (W.T).dot(W).dot(H)\n",
    "            \n",
    "            i += 1\n",
    "            \n",
    "        return W, H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = MyNMF()\n",
    "W, H = model.fit(X, 3, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00000000e+00, 0.00000000e+00, 1.28595080e+00],\n",
       "       [1.28345362e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [1.05790587e-01, 2.67463988e-01, 4.49034869e-02],\n",
       "       [4.42746168e-09, 1.68311835e+00, 3.97986442e-10],\n",
       "       [0.00000000e+00, 3.73906428e-01, 0.00000000e+00],\n",
       "       [1.43275413e+00, 7.52576889e-01, 6.79401001e-01]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.05124504e-06, 1.46377751e+00, 3.71958802e-01, 4.73580536e-25],\n",
       "       [3.74460106e-08, 1.19697283e-08, 1.28408868e+00, 1.71047314e+00],\n",
       "       [1.53560867e+00, 1.55308346e-06, 1.73756527e-01, 1.48990252e-25]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.97471720e+00, 1.99718892e-06, 2.23442344e-01, 1.91594134e-25],\n",
       "       [2.63267787e-06, 1.87869054e+00, 4.77391871e-01, 6.07818654e-25],\n",
       "       [6.89544109e-02, 1.54853954e-01, 3.90599494e-01, 4.57489968e-01],\n",
       "       [6.36372282e-08, 2.66272889e-08, 2.16127323e+00, 2.87892874e+00],\n",
       "       [1.40013040e-08, 4.47555835e-09, 4.80129012e-01, 6.39556902e-01],\n",
       "       [1.04329704e+00, 2.09723433e+00, 1.61735133e+00, 1.28726256e+00]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_ = inverse_transform(W, H);X_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.5439645173226346"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss(X, X_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用 sklearn 计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "model = NMF(n_components=3, init='random', max_iter=200, random_state=0)\n",
    "W = model.fit_transform(X)\n",
    "H = model.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.53849498, 0.        ],\n",
       "       [0.        , 1.07698996, 0.        ],\n",
       "       [0.69891361, 0.        , 0.        ],\n",
       "       [1.39782972, 0.        , 1.97173859],\n",
       "       [0.        , 0.        , 0.65783848],\n",
       "       [1.39783002, 1.34623756, 0.65573258]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00000000e+00, 0.00000000e+00, 1.43078959e+00, 1.71761682e-03],\n",
       "       [7.42810976e-01, 1.48562195e+00, 0.00000000e+00, 3.30264644e-04],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 1.52030365e+00]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.99999983e-01, 7.99999966e-01, 0.00000000e+00, 1.77845853e-04],\n",
       "       [7.99999966e-01, 1.59999993e+00, 0.00000000e+00, 3.55691707e-04],\n",
       "       [0.00000000e+00, 0.00000000e+00, 9.99998311e-01, 1.20046577e-03],\n",
       "       [0.00000000e+00, 0.00000000e+00, 2.00000021e+00, 3.00004230e+00],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 1.00011424e+00],\n",
       "       [1.00000003e+00, 2.00000007e+00, 2.00000064e+00, 9.99758185e-01]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X__ = inverse_transform(W, H);X__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.0000016725824565"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss(X, X__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
