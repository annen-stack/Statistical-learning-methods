{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.概率潜在语义分析是利用概率生成模型对文本集合进行话题分析的方法。概率潜在语义分析受潜在语义分析的启发提出两者可以通过矩阵分解关联起来。\n",
    "\n",
    "给定一个文本集合，通过概率潜在语义分析，可以得到各个文本生成话题的条件概率分布，以及各个话题生成单词的条件概率分布。\n",
    "\n",
    "概率潜在语义分析的模型有生成模型，以及等价的共现模型。其学习策略是观测数据的极大似然估计，其学习算法是EM算法。\n",
    "\n",
    "2.生成模型表示文本生成话题，话题生成单词从而得到单词文本共现数据的过程；假设每个文本由一个话题分布决定，每个话题由一个单词分布决定。单词变量$w$与文本变量$d$是观测变量话题变量$z$是隐变量。生成模型的定义如下：\n",
    "$$P ( T ) = \\prod _ { ( w , d ) } P ( w , d ) ^ { n ( w , d ) }$$\n",
    "\n",
    "$$P ( w , d ) = P ( d ) P ( w | d ) = P ( d ) \\sum _ { \\alpha } P ( z | d ) P ( w | z )$$\n",
    "3.共现模型描述文本单词共现数据拥有的模式。共现模型的定义如下：\n",
    "$$P ( T ) = \\prod _ { ( w , d ) } P ( w , d ) ^ { n ( w , d ) }$$\n",
    "\n",
    "$$P ( w , d ) = \\sum _ { z \\in Z } P ( z ) P ( w | z ) P ( d | z )$$\n",
    "\n",
    "4.概率潜在语义分析的模型的参数个数是$O ( M \\cdot K + N \\cdot K )$。现实中$K \\ll M$，所以概率潜在语义分析通过话题对数据进行了更简洁地表示，实现了数据压缩。\n",
    "\n",
    "5.模型中的概率分布$P ( w | d )$可以由参数空间中的单纯形表示。$M$维参数空间中，单词单纯形表示所有可能的文本的分布，在其中的话题单纯形表示在$K$个话题定义下的所有可能的文本的分布。话题单纯形是单词单纯形的子集，表示潜在语义空间。\n",
    "\n",
    "6.概率潜在语义分析的学习通常采用EM算法通过迭代学习模型的参数，$P ( w | z )$\n",
    "和$P ( z| d )$，而$P（d）$可直接统计得出。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "概率潜在语义分析（probabilistic latent semantic analysis, PLSA）,也称概率潜在语义索引（probabilistic latent semantic indexing, PLSI）,是一种利用概率生成模型对文本集合进行话题分析的无监督学习方法。\n",
    "\n",
    "模型最大特点是用隐变量表示话题，整个模型表示文本生成话题，话题生成单词，从而得到单词-文本共现数据的过程；假设每个文本由一个话题分布决定，每个话题由一个单词分布决定。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **18.1.2 生成模型**\n",
    "\n",
    "假设有单词集合 $W = $ {$w_{1}, w_{2}, ..., w_{M}$}， 其中M是单词个数；文本（指标）集合$D = $ {$d_{1}, d_{2}, ..., d_{N}$}, 其中N是文本个数；话题集合$Z = $ {$z_{1}, z_{2}, ..., z_{K}$}，其中$K$是预先设定的话题个数。随机变量 $w$ 取值于单词集合；随机变量 $d$ 取值于文本集合，随机变量 $z$ 取值于话题集合。概率分布 $P(d)$、条件概率分布 $P(z|d)$、条件概率分布 $P(w|z)$ 皆属于多项分布，其中 $P(d)$ 表示生成文本 $d$ 的概率，$P(z|d)$ 表示文本 $d$ 生成话题 $z$ 的概率，$P(w|z)$ 表示话题 $z$ 生成单词 $w$ 的概率。\n",
    "\n",
    "   每个文本 $d$ 拥有自己的话题概率分布 $P(z|d)$，每个话题 $z$ 拥有自己的单词概率分布 $P(w|z)$；也就是说**一个文本的内容由其相关话题决定，一个话题的内容由其相关单词决定**。\n",
    "   \n",
    "   生成模型通过以下步骤生成文本·单词共现数据：  \n",
    "   （1）依据概率分布 $P(d)$，从文本（指标）集合中随机选取一个文本 $d$ , 共生成 $N$  个文本；针对每个文本，执行以下操作；  \n",
    "   （2）在文本$d$ 给定条件下，依据条件概率分布 $P(z|d)$, 从话题集合随机选取一个话题 $z$, 共生成 $L$ 个话题，这里 $L$ 是文本长度；  \n",
    "   （3）在话题 $z$ 给定条件下，依据条件概率分布 $P(w|z)$ , 从单词集合中随机选取一个单词 $w$. \n",
    "   \n",
    " 注意这里为叙述方便，假设文本都是等长的，现实中不需要这个假设。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 算法 18.1 （概率潜在语义模型参数估计的EM算法）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "输入： 设单词集合为 $W = ${$w_{1}, w_{2},..., w_{M}$}, 文本集合为 $D=${$d_{1}, d_{2},..., d_{N}$}, 话题集合为 $Z=${$z_{1}, z_{2},..., z_{K}$}, 共现数据 $\\left \\{ n(w_{i}, d_{j}) \\right \\}, i = 1,2,..., M, j = 1,2,...,N;$  \n",
    "\n",
    "输出： $P(w_{i}|z_{k})$ 和 $P(z_{k}|d_{j})$.\n",
    "\n",
    "1. 设置参数 $P(w_{i}|z_{k})$ 和 $P(z_{k}|d_{j})$ 的初始值。\n",
    "\n",
    "2. 迭代执行以下E步，M步，直到收敛为止。  \n",
    "\n",
    " E步：  \n",
    "    $P(z_{k}|w_{i},d_{j})=\\frac{P(w_{i}|z_{k})P(z_{k}|d_{j})}{\\sum_{k=1}^{K}P(w_{i}|z_{k})P(z_{k}|d_{j})}$  \n",
    "  \n",
    " M步：  \n",
    "    $P(w_{i}|z_{k})=\\frac{\\sum_{j=1}^{N}n(w_{i},d_{j})P(z_{k}|w_{i},d_{j})}{\\sum_{m=1}^{M}\\sum_{j=1}^{N}n(w_{m},d_{j})P(z_{k}|w_{m},d_{j})}$  \n",
    "    \n",
    "    $P(z_{k}|d_{j}) = \\frac{\\sum_{i=1}^{M}n(w_{i},d_{j})P(z_{k}|w_{i},d_{j})}{n(d_{j})}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = [[0,0,1,1,0,0,0,0,0], \n",
    "     [0,0,0,0,0,1,0,0,1], \n",
    "     [0,1,0,0,0,0,0,1,0], \n",
    "     [0,0,0,0,0,0,1,0,1], \n",
    "     [1,0,0,0,0,1,0,0,0], \n",
    "     [1,1,1,1,1,1,1,1,1], \n",
    "     [1,0,1,0,0,0,0,0,0], \n",
    "     [0,0,0,0,0,0,1,0,1], \n",
    "     [0,0,0,0,0,2,0,0,1], \n",
    "     [1,0,1,0,0,0,0,1,0], \n",
    "     [0,0,0,1,1,0,0,0,0]]\n",
    "X = np.asarray(X);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1, 1, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 1, 0, 0, 1],\n",
       "       [0, 1, 0, 0, 0, 0, 0, 1, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 1, 0, 1],\n",
       "       [1, 0, 0, 0, 0, 1, 0, 0, 0],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "       [1, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 1, 0, 1],\n",
       "       [0, 0, 0, 0, 0, 2, 0, 0, 1],\n",
       "       [1, 0, 1, 0, 0, 0, 0, 1, 0],\n",
       "       [0, 0, 0, 1, 1, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class PLSA:\n",
    "    def __init__(self, K, max_iter):\n",
    "        self.K = K\n",
    "        self.max_iter = max_iter\n",
    "\n",
    "    def fit(self, X):\n",
    "        n_d, n_w = X.shape\n",
    "\n",
    "        # P(z|w,d)\n",
    "        p_z_dw = np.zeros((n_d, n_w, self.K))\n",
    "\n",
    "        # P(z|d)\n",
    "        p_z_d = np.random.rand(n_d, self.K)\n",
    "\n",
    "        # P(w|z)\n",
    "        p_w_z = np.random.rand(self.K, n_w)\n",
    "\n",
    "        for i_iter in range(self.max_iter):\n",
    "            # E step\n",
    "            for di in range(n_d):\n",
    "                for wi in range(n_w):\n",
    "                    sum_zk = np.zeros((self.K))\n",
    "                    for zi in range(self.K):\n",
    "                        sum_zk[zi] = p_z_d[di, zi] * p_w_z[zi, wi]\n",
    "                    sum1 = np.sum(sum_zk)\n",
    "                    if sum1 == 0:\n",
    "                        sum1 = 1\n",
    "                    for zi in range(self.K):\n",
    "                        p_z_dw[di, wi, zi] = sum_zk[zi] / sum1\n",
    "\n",
    "            # M step\n",
    "\n",
    "            # update P(z|d)\n",
    "            for di in range(n_d):\n",
    "                for zi in range(self.K):\n",
    "                    sum1 = 0.\n",
    "                    sum2 = 0.\n",
    "\n",
    "                    for wi in range(n_w):\n",
    "                        sum1 = sum1 + X[di, wi] * p_z_dw[di, wi, zi]\n",
    "                        sum2 = sum2 + X[di, wi]\n",
    "\n",
    "                    if sum2 == 0:\n",
    "                        sum2 = 1\n",
    "                    p_z_d[di, zi] = sum1 / sum2\n",
    "\n",
    "            # update P(w|z)\n",
    "            for zi in range(self.K):\n",
    "                sum2 = np.zeros((n_w))\n",
    "                for wi in range(n_w):\n",
    "                    for di in range(n_d):\n",
    "                        sum2[wi] = sum2[wi] + X[di, wi] * p_z_dw[di, wi, zi]\n",
    "                sum1 = np.sum(sum2)\n",
    "                if sum1 == 0:\n",
    "                    sum1 = 1\n",
    "                    for wi in range(n_w):\n",
    "                        p_w_z[zi, wi] = sum2[wi] / sum1\n",
    "\n",
    "        return p_w_z, p_z_d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = PLSA(2, 100)\n",
    "p_w_z, p_z_d = model.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.23379312, 0.8829568 , 0.0386523 , 0.40286289, 0.20248982,\n",
       "        0.48178336, 0.70569096, 0.17066972, 0.14161982],\n",
       "       [0.32029458, 0.68543733, 0.71972551, 0.06833537, 0.16397667,\n",
       "        0.9948534 , 0.66993813, 0.04372268, 0.35325913]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_w_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.26238881e-01, 5.73761119e-01],\n",
       "       [1.00108494e-35, 1.00000000e+00],\n",
       "       [1.00000000e+00, 2.44506195e-29],\n",
       "       [8.83011477e-15, 1.00000000e+00],\n",
       "       [9.65635879e-23, 1.00000000e+00],\n",
       "       [4.32380435e-01, 5.67619565e-01],\n",
       "       [4.95513343e-42, 1.00000000e+00],\n",
       "       [9.37233870e-15, 1.00000000e+00],\n",
       "       [5.97881962e-35, 1.00000000e+00],\n",
       "       [2.85100460e-01, 7.14899540e-01],\n",
       "       [1.00000000e+00, 1.44743942e-32]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_z_d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
